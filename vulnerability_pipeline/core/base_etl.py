from abc import ABC, abstractmethod
from pymongo import UpdateOne
from datetime import datetime
from ..core.mongo_client import MongoManager
import logging

class BaseExtractor(ABC):
    def __init__(self, source_name):
        self.source_name = source_name
        self.logger = logging.getLogger(f"pipeline.{source_name}.extractor")

    @abstractmethod
    def extract(self, since=None):
        """
        Extract data from the source.
        :param since: Optional timestamp/version to filter data (incremental load).
        :return: Iterable of data items.
        """
        pass

class BaseLoader(ABC):
    def __init__(self, collection_name, key_field="id"):
        self.collection_name = collection_name
        self.key_field = key_field
        self.db = MongoManager.get_bronze_db()
        self.collection = self.db[collection_name]
        self.logger = logging.getLogger(f"pipeline.{collection_name}.loader")
        self._ensure_index()

    def _ensure_index(self):
        """Creates an index on the key field to optimize upserts."""
        try:
            self.collection.create_index([(self.key_field, 1)], unique=True)
            self.logger.info(f"[{self.collection_name}] Verified unique index on '{self.key_field}'")
        except Exception as e:
            self.logger.warning(f"[{self.collection_name}] Could not create index: {e}")

    def load(self, data):
        """
        Loads data into MongoDB using bulk upserts.
        :param data: List or Iterable of dictionaries.
        """
        if not data:
            self.logger.info(f"[{self.collection_name}] No data to load.")
            return

        operations = []
        stats = {
            "processed": 0,
            "upserted": 0,
            "modified": 0,
            "matched": 0
        }
        
        batch_size = 2000
        current_batch = []

        for item in data:
            doc = self.transform(item)
            if doc:
                # Ensure ingested_at is present
                if "ingested_at" not in doc:
                    doc["ingested_at"] = datetime.utcnow()
                
                op = UpdateOne({self.key_field: doc[self.key_field]}, {"$set": doc}, upsert=True)
                current_batch.append(op)
                
                if len(current_batch) >= batch_size:
                     self._execute_batch(current_batch, stats)
                     current_batch = []
        
        if current_batch:
            self._execute_batch(current_batch, stats)
            
        self.logger.info(f"[{self.collection_name}] Load Complete. Stats: {stats}")
        print(f"[{self.collection_name}] Load Summary: {stats['upserted']} inserted, {stats['modified']} updated, {stats['matched']} matched.")

    def _execute_batch(self, operations, stats):
        if operations:
            try:
                result = self.collection.bulk_write(operations)
                stats["processed"] += len(operations)
                stats["upserted"] += result.upserted_count
                stats["modified"] += result.modified_count
                stats["matched"] += result.matched_count
            except Exception as e:
                self.logger.error(f"[{self.collection_name}] Error during bulk write: {e}")

    def transform(self, item):
        """
        Optional hook to transform item before loading.
        Default is identity (return item as is).
        Override in subclass if specific mapping is needed.
        """
        return item
    
    def get_max_key(self, field_name):
        """
        Helper to get the maximum value of a field in the collection.
        Useful for determining the 'since' parameter for incremental loads.
        """
        try:
            # Sort descending and get one
            result = self.collection.find_one({}, sort=[(field_name, -1)])
            if result:
                return result.get(field_name)
        except Exception as e:
            print(f"[{self.collection_name}] Error getting max key: {e}")
        return None
