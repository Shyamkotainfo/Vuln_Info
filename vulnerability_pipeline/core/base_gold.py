import logging
import pymongo
from abc import ABC, abstractmethod
from datetime import datetime
from vulnerability_pipeline.core.mongo_client import MongoManager
from vulnerability_pipeline.gold.mapping_config import SOURCE_MIRROR_CONFIG

class BaseGoldPipeline(ABC):
    def __init__(self, target_collection):
        self.target_collection_name = target_collection
        self.silver_db = MongoManager.get_silver_db()
        self.gold_db = MongoManager.get_gold_db()
        self.target_col = self.gold_db[target_collection]
        self.logger = logging.getLogger(f"pipeline.gold.{target_collection}")

    def upsert_batch(self, docs, key_field="id"):
        if not docs:
            return
        
        operations = []
        now = datetime.utcnow()
        for doc in docs:
            doc['gold_updated_at'] = now
            # Assume key_field exists
            filter_q = {key_field: doc.get(key_field)}
            if not doc.get(key_field) and "_id" in doc:
                 filter_q = {"_id": doc["_id"]}

            update_doc = {"$set": doc, "$setOnInsert": {"gold_created_at": now}}
            operations.append(pymongo.UpdateOne(filter_q, update_doc, upsert=True))
        
        if operations:
            result = self.target_col.bulk_write(operations)
            self.logger.info(f"Gold Upsert: {result.upserted_count} new, {result.modified_count} updated.")

class GoldMirrorPipeline(BaseGoldPipeline):
    """
    Standard pipeline to key-mapped copy from Silver to Gold.
    """
    def __init__(self, source_name):
        self.source_name = source_name
        # Silver collection map
        silver_map = {
            "nvd": "nvd_silver",
            "cisa": "cisa_silver",
            "epss": "epss_silver",
            "exploit": "exploit_silver",
            "metasploit": "metasploit_silver"
        }
        self.silver_col_name = silver_map.get(source_name)
        target_name = f"gold_{source_name}"
        super().__init__(target_name)

    def run(self):
        self.logger.info(f"Starting Gold Mirror for {self.source_name}")
        silver_col = self.silver_db[self.silver_col_name]
        
        # Get Config
        config = SOURCE_MIRROR_CONFIG.get(self.source_name, {})
        include_cols = config.get("include")
        exclude_cols = config.get("exclude", [])
        manual_cols = config.get("manual", {})

        batch = []
        BATCH_SIZE = 1000
        
        # Incremental check could be added here (using gold_updated_at vs silver_updated_at)
        # For now, we do full sync as requested or key-based.
        # Efficient approach: Stream all Silver.
        
        cursor = silver_col.find({})
        for doc in cursor:
            # 1. Filter Columns
            new_doc = {}
            
            # If include is defined, only take those
            if include_cols:
                for key in include_cols:
                    if key in doc:
                        new_doc[key] = doc[key]
                # Always keep _id and key join field?
                if "_id" not in new_doc: new_doc["_id"] = doc["_id"]
                if "id" in doc and "id" not in new_doc: new_doc["id"] = doc["id"]
            else:
                # Take all except excluded
                for k, v in doc.items():
                    if k not in exclude_cols:
                        new_doc[k] = v
            
            # 2. Add Manual Columns
            for m_key, m_val in manual_cols.items():
                new_doc[m_key] = m_val

            batch.append(new_doc)
            
            if len(batch) >= BATCH_SIZE:
                self.upsert_batch(batch, key_field="id" if "id" in batch[0] else "_id")
                batch = []
        
        if batch:
            self.upsert_batch(batch, key_field="id" if "id" in batch[0] else "_id")

import hashlib
from vulnerability_pipeline.gold import mapping_config

# ... (Previous BaseGoldPipeline code) ...

class DimensionalGoldPipeline(BaseGoldPipeline):
    """
    Generic pipeline to transform Silver Mirrors into Long-Format Dimensional Tables.
    Schema: {s_no (hash), cve_id, factor_name, value, category, static_or_dynamic, ...}
    """
    def __init__(self, target_collection, config_key):
        super().__init__(target_collection)
        self.config_key = config_key
    
    def _get_value_by_path(self, doc, path):
        """Helper to get nested value or constant."""
        if path == "TRUE": return True
        
        parts = path.split('.')
        val = doc
        for p in parts:
            if isinstance(val, dict):
                val = val.get(p)
            elif isinstance(val, list) and p.isdigit():
                try: 
                    val = val[int(p)]
                except: 
                    return None
            else:
                return None
        return val

    def run(self):
        self.logger.info(f"Starting Dimensional Aggregation for {self.target_collection_name}...")
        
        # Get Factor Config (e.g., VRR_FACTORS, THREAT_FACTORS)
        factors_config = getattr(mapping_config, self.config_key, {})
        
        batch = []
        BATCH_SIZE = 1000
        
        # Iterate through configured sources
        for source_name, factors in factors_config.items():
            col_name = f"gold_{source_name}"
            source_col = self.gold_db[col_name]
            
            self.logger.info(f"Processing Factors from {col_name}...")
            
            cursor = source_col.find({})
            for doc in cursor:
                cve_id = doc.get('id') or doc.get('cveID') or doc.get('cve_id')
                if not cve_id: continue
                
                # For each factor defined for this source
                for factor_key, config in factors.items():
                    val = self._get_value_by_path(doc, config['path'])
                    
                    if val is not None:
                        # Create Dimensional Row
                        # Unique ID = Hash(CVE + Factor + TargetTable)
                        row_id_str = f"{cve_id}_{factor_key}_{self.target_collection_name}"
                        row_id = hashlib.md5(row_id_str.encode()).hexdigest()
                        
                        row = {
                            "_id": row_id,
                            "s_no": row_id,
                            "cve_id": cve_id,
                            "factor_name": factor_key,
                            "value": val,
                            "category": config.get('category', 'General'),
                            "static_or_dynamic": config.get('type', 'Dynamic'),
                            "source": source_name,
                            # Dates added by base upsert logic
                        }
                        batch.append(row)
                
                if len(batch) >= BATCH_SIZE:
                    self.upsert_batch(batch, key_field="_id")
                    batch = []
        
        if batch:
            self.upsert_batch(batch, key_field="_id")
            
        self.logger.info(f"Aggregation {self.target_collection_name} Complete.")
