import requests
import csv
import time
import random
import re
import concurrent.futures
from datetime import datetime
from bs4 import BeautifulSoup
from vulnerability_pipeline.core.base_etl import BaseExtractor

class ExploitDBExtractor(BaseExtractor):
    def __init__(self):
        super().__init__("ExploitDB")
        self.csv_url = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"
        self.base_url = "https://www.exploit-db.com/exploits"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"
        }
        # CONFIGURATION
        self.MAX_WORKERS = 100  # Increase to 50-100 for speed (Risk of IP Ban!)

    def _scrape_metadata(self, exploit_id):
        """Scrapes rich metadata from the Exploit-DB page."""
        try:
            url = f"{self.base_url}/{exploit_id}"
            # Sleep slightly to be polite even in threads
            time.sleep(random.uniform(0.5, 1.0))
            
            resp = requests.get(url, headers=self.headers, timeout=10)
            if resp.status_code != 200:
                return {}

            soup = BeautifulSoup(resp.content, 'html.parser')
            meta = {}

            # 1. Screenshot
            screenshot = soup.select_one("a.screenshot-url")
            meta["screenshot_url"] = screenshot["href"] if screenshot else None

            # 2. Verified Status
            verified_icon = soup.select_one("i.mdi-check") or soup.select_one("i.fa-check")
            meta["verified"] = True if verified_icon else False
            
            # 3. Source URL
            meta["source_url"] = url
            
            # 4. Application URL
            # Search for link with text "Vulnerable App" or "Download" inside specific container?
            # Or generic search for now.
            # EDB structure usually has a specific button. Let's try finding a link that says "Vulnerable App"
            # or href contains 'application'
            app_link = soup.find("a", string=lambda t: t and "Vulnerable App" in t)
            if not app_link:
                # Fallback: finding link to software downloads
                app_link = soup.find("a", href=lambda h: h and ("software" in h or "download" in h) and "exploit-db.com" not in h)
            meta["application_url"] = app_link["href"] if app_link else None

            # 5. Codes (CVE / OSVDB) and Tags
            # From <meta name="keywords" content="AIX,dos,CVE-2009-3699">
            meta["codes"] = []
            meta["tags"] = []
            
            keywords_tag = soup.find("meta", attrs={"name": "keywords"})
            if keywords_tag and keywords_tag.get("content"):
                keywords = [k.strip() for k in keywords_tag["content"].split(",")]
                for k in keywords:
                    # Check for CVE patterns
                    if re.match(r"(CVE-\d{4}-\d+|OSVDB-\d+)", k, re.IGNORECASE):
                        meta["codes"].append(k.upper())
                    else:
                        # It's a tag (clean up if needed)
                        if k and len(k) < 50: # valid tag length
                            meta["tags"].append(k)

            # Debug log
            if meta["verified"]:
                print(f"  [+] Exploit {exploit_id} Verified!")
            # EDB doesn't always show tags easily. Checking for 'meta name="keywords"'
            # Or codes (CVEs).
            # Codes are often in the title or separate block.
            # Simplify for now.
            
            return meta
        except Exception as e:
            # self.logger.warning(f"Failed to scrape {exploit_id}: {e}")
            return {}

    def _process_row(self, row):
        """Helper to scrape and merge data for a single row."""
        try:
            print(f"Scraping metadata for Exploit ID: {row['id']}")
            enriched_meta = self._scrape_metadata(row['id'])
            row.update(enriched_meta)
            return row
        except Exception as e:
            print(f"Error processing row {row.get('id')}: {e}")
            return row

    def extract(self, since=None):
        try:
            print("Fetching ExploitDB CSV index...")
            response = requests.get(self.csv_url, timeout=30)
            response.raise_for_status()
            
            content = response.content.decode('utf-8')
            reader = csv.DictReader(content.splitlines())
            
            # 1. Collect rows that need processing
            rows_to_process = []
            for row in reader:
                date_val = row.get("date_updated") or row.get("date_published")
                if since and date_val and date_val <= since:
                    continue
                rows_to_process.append(row)
            
            total_docs = len(rows_to_process)
            print(f"Found {total_docs} exploits to scrape/enrich.")
            
            # 2. Parallel Processing
            # SAFETY LIMIT: If total > 1000 and user hasn't explicitly adjusted code, warn?
            # We enforce MAX_WORKERS.
            
            finished_count = 0
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
                # Submit all tasks
                future_to_row = {executor.submit(self._process_row, row): row for row in rows_to_process}
                
                for future in concurrent.futures.as_completed(future_to_row):
                    try:
                        result_row = future.result()
                        yield result_row
                        finished_count += 1
                        if finished_count % 100 == 0:
                            print(f"Progress: {finished_count}/{total_docs} scraped.")
                    except Exception as exc:
                        print(f"Scrape task generated an exception: {exc}")

            print(f"Yielded {finished_count} ExploitDB records (Enriched).")
                
        except Exception as e:
            print(f"Error extracting ExploitDB data: {e}")
            return []
