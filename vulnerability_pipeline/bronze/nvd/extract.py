import requests
import time
from datetime import datetime
from vulnerability_pipeline.core.base_etl import BaseExtractor

class NVDExtractor(BaseExtractor):
    def __init__(self, api_key=None):
        super().__init__("NVD")
        self.base_url = "https://services.nvd.nist.gov/rest/json/cves/2.0"
        self.api_key = api_key
        self.delay = 0.6 if api_key else 6.0 

    def extract(self, since=None):
        """
        Extracts NVD data.
        :param since: ISO8601 string for lastModStartDate (e.g., '2024-01-01T00:00:00.000').
                      If None, defaults to full history logic (handled via chunking if needed).
        """
        from datetime import datetime, timedelta

        # Default to a very old date if 'since' is None to simulate "Full Load" via incremental logic
        # NVD API requires both start and end if start is present.
        start_date = datetime.fromisoformat(since.replace('Z', '+00:00')) if since else datetime(1999, 1, 1) # NVD starts around 1999/2002
        end_date_limit = datetime.utcnow()
        
        # NVD Max Range is 120 days. We chunk it.
        # Check if we need chunking
        if (end_date_limit - start_date).days > 120:
            current_start = start_date
            while current_start < end_date_limit:
                current_end = min(current_start + timedelta(days=119), end_date_limit)
                
                # Format for API
                start_str = current_start.isoformat()
                end_str = current_end.isoformat()
                
                print(f"Fetch window: {start_str} to {end_str}")
                
                # Yield from the chunk fetcher
                yield from self._extract_chunk(start_str, end_str)
                
                current_start = current_end # Next chunk starts where this one ended? 
                # Actually NVD uses strict inequality? Overlap risk?
                # Ideally start next chunk at current_end + microsecond? 
                # Let's simple overlap or strict >. API includes start, includes end?
                # Standard convention: [start, end]. So next should be [end + small, ...]
                # or just overlap by a second and handle dedupe in Loader (Loader handles upsert).
                # Overlap is safer.
                current_start = current_end
                
        else:
            # Short range, just run once
            yield from self._extract_chunk(start_date.isoformat(), end_date_limit.isoformat())

    def _extract_chunk(self, start_str, end_str):
        return self.extract_all_concurrent(since=start_str, end=end_str)

    def extract_from_api(self, start_index=0, limit=2000, since=None, end=None):
        headers = {}
        if self.api_key:
            headers['apiKey'] = self.api_key
        params = {
            'resultsPerPage': limit,
            'startIndex': start_index
        }
        if since:
            params['lastModStartDate'] = since
        if end:
            params['lastModEndDate'] = end

        try:
            # print(f"Fetching startIndex={start_index}, limit={limit}...") # Very verbose
            response = requests.get(self.base_url, headers=headers, params=params, timeout=60)
            if response.status_code == 429:
                print(f"Rate limit hit at startIndex={start_index}. Retrying after sleep...")
                time.sleep(10)
                return self.extract_from_api(start_index, limit, since, end)

            response.raise_for_status()
            data = response.json()
            return data.get('vulnerabilities', []), data.get('totalResults', 0)
        except Exception as e:
            print(f"Error extracting NVD data at index {start_index}: {e}")
            return [], 0
    
    def extract_all_concurrent(self, batch_size=2000, max_workers=5, since=None, end=None):
        import concurrent.futures
        
        first_batch, total_results = self.extract_from_api(start_index=0, limit=1, since=since, end=end)
        if total_results == 0:
            return

        print(f"Total NVD CVEs in window {since} - {end}: {total_results}")
        
        offsets = range(0, total_results, batch_size)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_offset = {executor.submit(self.extract_from_api, offset, batch_size, since, end): offset for offset in offsets}
            
            for future in concurrent.futures.as_completed(future_to_offset):
                offset = future_to_offset[future]
                try:
                    data, _ = future.result()
                    if data:
                        yield from data
                    time.sleep(self.delay) 
                except Exception as exc:
                    print(f"Batch starting at {offset} generated an exception: {exc}")
